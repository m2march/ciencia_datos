{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parseamos TASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tasa', 'r') as f:\n",
    "    all_tasa = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasa_documents = [doc for doc in all_tasa.split('\\n\\n') if len(doc) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tas_doc_id(tasa_doc):\n",
    "    first_line = tasa_doc.split('\\n', 2)[0]\n",
    "    doc_id = re.match('\\[([a-zA-Z0-9\\.]*)\\]', first_line).groups()[0]\n",
    "    return doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punc_translator = str.maketrans(dict(zip(string.punctuation, [None] * len(string.punctuation))))\n",
    "def tokenize(string):\n",
    "    'Returns list of tokens'\n",
    "    # TODO: Cambiar split por: https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
    "    #words = string.split(' ')  # [Palabra]\n",
    "    retoken = [ t.lower()\n",
    "        for t in nltk.word_tokenize(string)\n",
    "    ] \n",
    "    no_punt = [ t\n",
    "        for t in retoken\n",
    "        if t.translate(punc_translator) != ''\n",
    "    ]\n",
    "    \n",
    "    return no_punt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tasa_doc_tokens(tasa_doc):\n",
    "    text = ' '.join([\n",
    "            l.strip().replace('\\n', '') \n",
    "            for l in tasa_doc.split('[S]')[1:]\n",
    "        ])\n",
    "    return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PICKLES_DIR = 'pickles'\n",
    "EJ3_TASA_TOKENS_PATH = os.path.join(PICKLES_DIR, 'tasa_tokens.pkl')\n",
    "if not os.path.isdir(PICKLES_DIR):\n",
    "    os.mkdir(PICKLES_DIR)\n",
    "\n",
    "if not os.path.isfile(EJ3_TASA_TOKENS_PATH):\n",
    "    tasa_tokens = []\n",
    "    for tasa_doc in tasa_documents:\n",
    "        tasa_tokens.append(\n",
    "            (get_tas_doc_id(tasa_doc), get_tasa_doc_tokens(tasa_doc))\n",
    "        )\n",
    "    tasa_tokens = dict(tasa_tokens)\n",
    "    \n",
    "    with open(EJ3_TASA_TOKENS_PATH, 'wb') as f:\n",
    "        pickle.dump(tasa_tokens, f)\n",
    "else:\n",
    "    with open(EJ3_TASA_TOKENS_PATH, 'rb') as f:\n",
    "        tasa_tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos LSI (LSA Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASA_LSI_PATH = os.path.join(PICKLES_DIR, 'tasa_lsi.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(TASA_LSI_PATH):\n",
    "    tasa_dict = gensim.corpora.Dictionary(tasa_tokens.values())\n",
    "    \n",
    "    tasa_corpus = { \n",
    "        doc_id : tasa_dict.doc2bow(tokens)\n",
    "        for doc_id, tokens in tasa_tokens.items()\n",
    "    }\n",
    "    \n",
    "    tasa_lsi = gensim.models.lsimodel.LsiModel(corpus=tasa_corpus.values(), \n",
    "                                               id2word=tasa_dict, \n",
    "                                               num_topics=300)\n",
    "    \n",
    "    tasa_lsi.save(TASA_LSI_PATH)\n",
    "else:\n",
    "    tasa_lsi = gensim.models.LsiModel.load(TASA_LSI_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parseamos WordSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ws = pd.DataFrame.from_csv('wordsim/combined.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.columns = ['w1', 'w2', 'human_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>human_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         w1        w2  human_dist\n",
       "0      love       sex        6.77\n",
       "1     tiger       cat        7.35\n",
       "2     tiger     tiger       10.00\n",
       "3      book     paper        7.46\n",
       "4  computer  keyboard        7.62"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tasa_dist(word_1, word_2, tasa_dict=tasa_dict, tasa_lsi=tasa_lsi):\n",
    "    w_id_1 = tasa_dict.doc2bow([word_1])\n",
    "    w_id_2 = tasa_dict.doc2bow([word_2])\n",
    "    if len(w_id_1) == 0 or len(w_id_2) == 0:\n",
    "        return None\n",
    "    wv1 = tasa_lsi[w_id_1]\n",
    "    wv2 = tasa_lsi[w_id_2]\n",
    "    csc1 = gensim.matutils.corpus2dense([wv1], 300).reshape(-1)\n",
    "    csc2 = gensim.matutils.corpus2dense([wv2], 300).reshape(-1)\n",
    "    return ((1 + (1 - cosine(csc1, csc2))) / 2) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws['lsi'] = [tasa_dist(*r[1][['w1', 'w2']]) for r in ws.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TASA_WV_PATH = os.path.join(PICKLES_DIR, 'tasa_wv.wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(TASA_LSI_PATH):\n",
    "    tasa_wv = gensim.models.Word2Vec(list(tasa_tokens.values()), workers=4)\n",
    "    tasa_wv.save(TASA_WV_PATH)\n",
    "else:\n",
    "    tasa_wv = gensim.models.Word2Vec.load(TASA_WV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sim_wv = []\n",
    "for r in ws.iterrows():\n",
    "    w1, w2 = r[1][['w1', 'w2']]\n",
    "    try:\n",
    "        word_sim_wv.append(tasa_wv.similarity(w1, w2))\n",
    "    except KeyError:\n",
    "        word_sim_wv.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif_nan = np.nonzero(pd.isnull(ws['lsi']) != pd.isnull(ws['wv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.iloc[dif_nan]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
